{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is Keyword Extraction?\n",
    "Keyword extraction is defined as the task of Natural language processing that automatically identifies a set of terms to describe the subject of the text. This is an important method in information retrieval (IR) systems: keywords simplify and speed up research. Keyword extraction can be used to reduce text dimensionality for further text analysis (subject modeling text classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7241, 7)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>event_type</th>\n",
       "      <th>pdf_name</th>\n",
       "      <th>abstract</th>\n",
       "      <th>paper_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1987</td>\n",
       "      <td>Self-Organization of Associative Database and ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1-self-organization-of-associative-database-an...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>1987</td>\n",
       "      <td>A Mean Field Theory of Layer IV of Visual Cort...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10-a-mean-field-theory-of-layer-iv-of-visual-c...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100</td>\n",
       "      <td>1988</td>\n",
       "      <td>Storing Covariance by the Associative Long-Ter...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100-storing-covariance-by-the-associative-long...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1000</td>\n",
       "      <td>1994</td>\n",
       "      <td>Bayesian Query Construction for Neural Network...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1000-bayesian-query-construction-for-neural-ne...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Bayesian Query Construction for Neural\\nNetwor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1001</td>\n",
       "      <td>1994</td>\n",
       "      <td>Neural Network Ensembles, Cross Validation, an...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1001-neural-network-ensembles-cross-validation...</td>\n",
       "      <td>Abstract Missing</td>\n",
       "      <td>Neural Network Ensembles, Cross\\nValidation, a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  year                                              title event_type  \\\n",
       "0     1  1987  Self-Organization of Associative Database and ...        NaN   \n",
       "1    10  1987  A Mean Field Theory of Layer IV of Visual Cort...        NaN   \n",
       "2   100  1988  Storing Covariance by the Associative Long-Ter...        NaN   \n",
       "3  1000  1994  Bayesian Query Construction for Neural Network...        NaN   \n",
       "4  1001  1994  Neural Network Ensembles, Cross Validation, an...        NaN   \n",
       "\n",
       "                                            pdf_name          abstract  \\\n",
       "0  1-self-organization-of-associative-database-an...  Abstract Missing   \n",
       "1  10-a-mean-field-theory-of-layer-iv-of-visual-c...  Abstract Missing   \n",
       "2  100-storing-covariance-by-the-associative-long...  Abstract Missing   \n",
       "3  1000-bayesian-query-construction-for-neural-ne...  Abstract Missing   \n",
       "4  1001-neural-network-ensembles-cross-validation...  Abstract Missing   \n",
       "\n",
       "                                          paper_text  \n",
       "0  767\\n\\nSELF-ORGANIZATION OF ASSOCIATIVE DATABA...  \n",
       "1  683\\n\\nA MEAN FIELD THEORY OF LAYER IV OF VISU...  \n",
       "2  394\\n\\nSTORING COVARIANCE BY THE ASSOCIATIVE\\n...  \n",
       "3  Bayesian Query Construction for Neural\\nNetwor...  \n",
       "4  Neural Network Ensembles, Cross\\nValidation, a...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "df = pd.read_csv(r'C:\\Users\\amany\\Desktop\\archive datasets\\papers.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### preprocess textual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "#creating a list of custom stopwords\n",
    "new_words=['fig','figure','image','sample','show','result','large','using','also','one','two','three','four','five','six',\n",
    "          'seven','eight','nine']\n",
    "stopwords=list(stopwords.union(new_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       767 self organization associative database app...\n",
       "1       683 mean field theory layer iv visual cortex a...\n",
       "2       394 storing covariance associative long term p...\n",
       "3       bayesian query construction neural network mod...\n",
       "4       neural network ensemble cross validation activ...\n",
       "                              ...                        \n",
       "7236    single transistor learning synapsis paul hasle...\n",
       "7237    bias variance combination least square estimat...\n",
       "7238    real time clustering cmos neural engine serran...\n",
       "7239    learning direction global motion class psychop...\n",
       "7240    correlation interpolation network real time ex...\n",
       "Name: paper_text, Length: 7241, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm=WordNetLemmatizer()\n",
    "def preprocess(text):\n",
    "    text=text.lower()\n",
    "    text=re.sub('[^A-Za-z0-9]',' ',text)\n",
    "    text=text.split()\n",
    "    text=[lm.lemmatize(word) for word in text if word not in stopwords]\n",
    "    return ' '.join(text)\n",
    "docs=df['paper_text'].apply(lambda x:preprocess(x))\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'767 self organization associative database application hisashi suzuki suguru arimoto osaka university toyonaka osaka 560 japan abstract efficient method self organizing associative database proposed together application robot eyesight system proposed database associate input output first half part discussion algorithm self organization proposed aspect hardware produce new style neural network latter half part applicability handwritten letter recognition autonomous mobile robot system demonstrated introduction let mapping f x given x finite infinite set another finite infinite set learning machine observes set pair x sampled randomly x x x x mean cartesian product x computes estimate j x f make small estimation error measure usually say faster decrease estimation error increase number sample better learning machine however expression performance incomplete since lack consideration candidate j j assumed preliminarily find good learning machine clarify conception let u discus type learning machine let u advance understanding self organization associative database parameter type ordinary type learning machine assumes equation relating x parameter indefinite namely structure f equivalent define implicitly set f candidate f subset mapping x computes value parameter based observed sample call type parameter type learning machine defined well f 3 f j approach f number sample increase alternative case however estimation error remains eternally thus problem designing learning machine return find proper structure f sense hand assumed structure f demanded compact possible achieve fast learning word number parameter small since parameter j uniquely determined even though observed sample however demand proper contradicts compact consequently parameter type better compactness assumed structure proper better learning machine elementary conception design learning machine 1 universality ordinary neural network suppose sufficient knowledge f given though j unknown case comparatively easy find proper compact structure j alternative case however sometimes difficult possible solution give compactness assume almighty structure cover various 1 combination orthogonal base infinite dimension structure neural network 1 2 approximation obtained truncating finitely dimension implementation american institute physic 1988 768 main topic designing neural network establish desirable structure 1 work includes developing practical procedure compute value coefficient observed sample discussion flourishing since 1980 many efficient method proposed recently even hardware unit computing coefficient parallel speed sold e g anza mark iii odyssey e 1 nevertheless neural network always exists danger error remaining eternally estimating precisely speaking suppose combination base finite number define structure 1 essentially word suppose f 3 1 located near f case estimation error none negligible however 1 distant f estimation error never becomes negligible indeed many research report following situation appears 1 complex estimation error converges value 0 number sample increase decrease hardly even though dimension heighten property sometimes considerable defect neural network recursi type recursive type founded another methodology learning follows initial stage set fa instead notation f candidate equal set mapping x observing first xl yl e x x fa reduced fi xt yl e f observing second x2 y2 e x x fl reduced f2 xt yl x2 y2 e f thus candidate set f becomes gradually small observation sample proceeds observing sample write likelihood estimation 1 selected fi hence contrarily parameter type recursive type guarantee surely j approach 1 number sample increase recursive type observes x yd rewrite value 1 l x x x correlated hence type architecture composed rule rewriting free memory space architecture form naturally kind database build management system data self organizing way however database differs ordinary one following sense record sample already observed computes estimation l x x e x call database associative database first subject constructing associative database establish rule rewri ting purpose adap measure called dissimilari ty dissimilari ty mean mapping x x x real x x e x x x x x 0 whenever l x x however necessarily defined single formula definable example collection rule written form dissimilarity defines structure 1 locally x x hence even though knowledge f imperfect flect heuristic way hence contrarily neural network possible accelerate speed learning establishing well especially easily find simple l process analogically information like human see application paper recursive type show strongly effectiveness denote sequence observed sample xl yd x2 y2 simplest construction associative database observing sample 1 2 follows algorithm 1 initial stage let empty set every 1 2 let l x x e x equal x e l x x min e x x furthermore add x l produce sa e l u x 1 n 769 another version improved economize memory follows algorithm 2 initial stage let composed arbitrary element x x every 1 2 let ii lex x e x equal x e si l x x min x x e l furthermore ii l xi yi let si si l add xi yi si l produce si e si si l u xi yi either construction ii approach f increase however computation time grows proportionally size si second subject constructing associative database addressing rule employ economize computation time subsequent chapter construction associative database purpose proposed manages data form binary tree self organization associative database given sequence xl yl x2 y2 algorithm constructing associative database follows algorithm 3 step initialization let x root root xl yd x variable assigned respective node memorize data furthermore let 1 step 2 increase 1 put x reset pointer n root repeat following n arrives terminal node e leaf notation nand xt x n let n n mean descendant node n n otherwise let n n x r n step 3 display yin related information next put yin back step 2 otherwise first establish new descendant node n n secondly let x n yin x n yin x n yin xt 2 3 finally back step 2 loop step 2 3 stopped time continued suppose gate element namely artificial synapsis play role branching prepared obtain new style neural network gate element randomly connected algorithm letter recognition recen tly vertical slitting method recognizing typographic english letters3 elastic matching method recognizing hand written discrete english letters4 global training fuzzy logic search method recognizing chinese character written square style etc published self organization associative database realizes recognition handwritten continuous english letter 770 9 wn nov xk la dw1lo 4 4fig 1 source document 2 loo h 2 windowing 1000 2000 3000 4000 number sample 1000 2000 3000 4000 nualber sampl e 3 experiment scanner take document 1 letter recognizer us parallelogram window least cover maximal letter 2 process sequence letter shifting window recognizer scan word slant direction place window left vicinity may first black point detected window catch letter part succeeding letter recognition head letter performed end position namely boundary line letter becomes known hence starting scanning boundary repeating operation recognizer accomplishes recursively task thus major problem come identifying head letter window considering define following regard window image x define x accordingly x x e x x x denote b black point left area boundary window x project b onto window x measure euclidean distance 6 fj black point b x closest b let x x summation 6 black point b x divided number b regard couple reading position boundary define accordingly operator teach recognizer interaction relation window reading boundary algorithm 3 precisely recalled reading incorrect operator teach correct reading via console moreover boundary position incorrect teach correct position via mouse 1 show partially document used experiment 3 show change number node recognition rate defined relative frequency correct answer past 1000 trial speciiications window height 20dot width 10dot slant angular 68deg example level tree distributed 6 19 time 4000 recognition rate converged 74 experimentally recognition rate converges 60 85 case 95 rare case however attain 100 since e g c e distinguishable excessive lluctuation writing consistency x relation assured like number node increase endlessly 3 hence clever stop learning recognition rate attains upper limit improve recognition rate must consider spelling word future subject 771 obstacle avoiding movement various system camera type autonomous mobile robot reported flourishingly6 1o system made author 4 belongs category mathematical methodology solve usually problem obstacle avoiding movement cost minimization problem cost criterion established artificially contrarily self organization associative database reproduces faithfully cost criterion operator therefore motion robot learning becomes natural length width height robot 7m weight 30kg visual angle camera 55deg robot following factor motion turn le 30deg advance le 1m control speed le 3km h experiment done passageway wid th 2 5m inside building author laboratory exist 5 experimental intention arrange box smoking stand gas cylinder stool handcart etc passage way random let robot take camera recall similar trace route preliminarily recorded purpose define following let camera face 28deg downward take process low pas filter scanning vertically filtered bottom top search first point c luminance change excessively su bstitu te point bottom c white point c top black 6 obstacle exists front robot white area show free area robot move around regard binary 32 x 32dot image processed thus x define x accordingly every x x e x x x let x x number black point exclusive x x regard image obtained drawing route image x define accordingly robot superimposes current camera x route recalled x inquires operator instruction operator judge subjectively whether suggested route appropriate negative answer draw desirable route x mouse teach new robot opera tion defines implicitly sequence x reflecting cost criterion operator l iibube 22 11 roan 12 13 stationary uni 4 configuration autonomous mobile robot system 23 24 north 14 rmbi ie unit robot roan 5 experimental environment 772 wall camera preprocessing fa preprocessing 0 course suggest ion search 6 processing obstacle avoiding movement x 1 processing position identification define satisfaction rate relative frequency acceptable suggestion route past 100 trial typical experiment change satisfaction rate showed similar tendency 3 attains 95 around time 800 notice rest 5 mean directly percentage collision practice prevent collision adopting supplementary measure time 800 number node 145 level tree distributed 6 17 proposed method reflects delicately various character operator example robot trained operator 0 move slowly enough space obstacle trained another operator 0 brush quickly obstacle fact give u hint method printing character machine position identification robot identify position recalling similar landscape position data camera purpose principle suffices regard camera image position data x respectively however memory capacity finite actual compu ters hence cannot compress camera image slight loss information compression admittable long precision position identification acceptable area thus major problem come find suitable compression method experimental environment 5 jut passageway interval 3 6m section adjacent jut door robot identifies roughly surrounding landscape section place us temporarily triangular surveying technique exact measure necessary realize former task define following turn camera take panorama 360deg scanning horizontally center line substitute point luminance excessively change black point white 1 regard binary 360dot line image processed thus x define x accordingly every x x e x x x project black point x onto x measure euclidean distance 6 black point x closest let summation 6 similarly calculate exchanging role x x denoting number respectively nand n define 773 x x 2 n n 4 regard positive integer labeled section cf 5 define accordingly learning mode robot check exactly position counter reset periodically operator robot run arbitrarily passageway within 18m area learns relation landscape position data position identification beyond 18m area achieved crossing plural database another task automatic excepting periodic reset counter namely kind learning without teacher define identification rate relative frequency correct recall position data past 100 trial typical example converged 83 around time 400 time 400 number level 202 level oftree distributed 522 since identification failure 17 rejected considering trajectory pro blem arises practical use order improve identification rate compression ratio camera image must loosened possibility depends improvement hardware future 8 show example actual motion robot based database obstacle avoiding movement position identification example corresponds case moving 14 23 5 time interval per frame 40sec 1 ii 8 actual motion robot 774 conclusion method self organizing associative database proposed application robot eyesight system machine decomposes global structure unknown set local structure known learns universally input output response framework problem implies wide application area example shown paper defect algorithm 3 self organization tree balanced well subclass structure f subject imposed u widen class probable solution abolish addressing rule depending directly value instead establish another rule depending distribution function value investigation reference 1 hopfield j j w tank computing neural circuit model science 233 1986 pp 625 633 2 rumelhart e et al learning representation back propagating error nature 323 1986 pp 533 536 3 hull j j hypothesis generation computational model visual word recognition ieee expert fall 1986 pp 63 70 4 kurtzberg j feature analysis symbol recognition elastic matching ibm j re develop 31 1 1987 pp 91 95 5 wang q r c suen tree classifier heuristic search global training ieee trans pattern anal mach intell pami 9 1 1987 pp 91 102 6 brook r et al self calibration motion stereo vision mobile robot 4th int symp robotics research 1987 pp 267 276 7 goto stentz cmu system mobile robot navigation 1987 ieee int conf robotics automation 1987 pp 99 105 8 madarasz r et al design autonomous vehicle disabled ieee jour robotics automation ra 2 3 1986 pp 117 125 9 triendl e j kriegman stereo vision navigation within building 1987 ieee int conf robotics automation 1987 pp 1725 1730 10 turk et al video road following autonomous land vehicle 1987 ieee int conf robotics automation 1987 pp 273 279'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using TF-IDF\n",
    "TF-IDF stands for Text Frequency Inverse Document Frequency. The importance of each word increases in proportion to the number of times a word appears in the document (Text Frequency – TF) but is offset by the frequency of the word in the corpus (Inverse Document Frequency – IDF).\n",
    "\n",
    "Using the tf-idf weighting scheme, the keywords are the words with the highest TF-IDF score. For this task, I’ll first use the CountVectorizer method in Scikit-learn to create a vocabulary and generate the word count:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<7241x10000 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 6447304 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv=CountVectorizer(max_df=0.95,         # ignore words that appear in 95% of documents\n",
    "                   max_features=10000,  # the size of the vocabulary\n",
    "                   ngram_range=(1,3)    # vocabulary contains single words, bigrams, trigrams\n",
    "                  )\n",
    "word_count_vector=cv.fit_transform(docs)\n",
    "word_count_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I’m going to use the TfidfTransformer in Scikit-learn to calculate the reverse frequency of documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfTransformer(norm='l2', smooth_idf=True, sublinear_tf=False, use_idf=True)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidftransformer = TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidftransformer.fit(word_count_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Title=====\n",
      "Algorithms for Non-negative Matrix Factorization\n",
      "\n",
      "=====Abstract=====\n",
      "Non-negative matrix factorization (NMF) has previously been shown to \n",
      "be a useful decomposition for multivariate data. Two different multi- \n",
      "plicative algorithms for NMF are analyzed. They differ only slightly in \n",
      "the multiplicative factor used in the update rules. One algorithm can be \n",
      "shown to minimize the conventional least squares error while the other \n",
      "minimizes the generalized Kullback-Leibler divergence. The monotonic \n",
      "convergence of both algorithms can be proven using an auxiliary func- \n",
      "tion analogous to that used for proving convergence of the Expectation- \n",
      "Maximization algorithm. The algorithms can also be interpreted as diag- \n",
      "onally rescaled gradient descent, where the rescaling factor is optimally \n",
      "chosen to ensure convergence. \n",
      "\n",
      "===Keywords===\n",
      "ht 0.651\n",
      "ht ht 0.262\n",
      "update rule 0.238\n",
      "update 0.197\n",
      "auxiliary 0.146\n",
      "non negative matrix 0.146\n",
      "negative matrix 0.145\n",
      "rule 0.133\n",
      "nmf 0.126\n",
      "multiplicative 0.121\n"
     ]
    }
   ],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "\n",
    "    for idx, score in sorted_items:\n",
    "        fname = feature_names[idx]\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results\n",
    "\n",
    "# get feature names\n",
    "feature_names=cv.get_feature_names()\n",
    "\n",
    "def get_keywords(idx, docs):\n",
    "\n",
    "    #generate tf-idf for the given document\n",
    "    tf_idf_vector=tfidftransformer.transform(cv.transform([docs[idx]]))\n",
    "\n",
    "    #sort the tf-idf vectors by descending order of scores\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "    #extract only the top n; n here is 10\n",
    "    keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "def print_results(idx,keywords, df):\n",
    "    # now print the results\n",
    "    print(\"\\n=====Title=====\")\n",
    "    print(df['title'][idx])\n",
    "    print(\"\\n=====Abstract=====\")\n",
    "    print(df['abstract'][idx])\n",
    "    print(\"\\n===Keywords===\")\n",
    "    for k in keywords:\n",
    "        print(k,keywords[k])\n",
    "idx=941\n",
    "keywords=get_keywords(idx, docs)\n",
    "print_results(idx,keywords, df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id                                                         1861\n",
       "year                                                       2000\n",
       "title          Algorithms for Non-negative Matrix Factorization\n",
       "event_type                                                  NaN\n",
       "pdf_name      1861-algorithms-for-non-negative-matrix-factor...\n",
       "abstract      Non-negative matrix factorization (NMF) has pr...\n",
       "paper_text    Algorithms for Non-negative Matrix\\nFactorizat...\n",
       "Name: 941, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[941,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
